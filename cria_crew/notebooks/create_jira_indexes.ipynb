{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIRA to Pinecone Vector Database with Vertex AI\n",
    "# This notebook connects to JIRA, extracts ticket information, and creates embeddings using Google's Vertex AI\n",
    "# The embeddings are stored in Pinecone for semantic search\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime, timezone\n",
    "from dataclasses import dataclass, field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# JIRA and API imports\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# Google Cloud and Vertex AI imports\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "# Pinecone imports\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Imports completed successfully!\")\n",
    "print(\"ğŸ“ Next: Configure your JIRA and Pinecone credentials in the environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JiraIndexConfig:\n",
    "    \"\"\"Configuration for JIRA ticket indexing with customizable fields\"\"\"\n",
    "    \n",
    "    # JIRA Connection Settings\n",
    "    jira_url: str = \"\"\n",
    "    jira_username: str = \"\"\n",
    "    jira_api_token: str = \"\"\n",
    "    \n",
    "    # Pinecone Settings\n",
    "    pinecone_api_key: str = \"\"\n",
    "    pinecone_index_name: str = \"jira-tickets\"\n",
    "    pinecone_environment: str = \"gcp-starter\"  # or your preferred environment\n",
    "    \n",
    "    # Google Cloud Settings\n",
    "    google_cloud_project: str = \"\"\n",
    "    \n",
    "    # Configurable JIRA Fields to Index\n",
    "    fields_to_index: List[str] = field(default_factory=lambda: [\n",
    "        \"title\",           # Issue summary/title\n",
    "        \"description\",     # Issue description\n",
    "        \"summary\",         # Alternative to title\n",
    "        \"created_date\",    # When the ticket was created\n",
    "        \"ticket_number\",   # JIRA key (e.g., PROJ-123)\n",
    "        \"ticket_id\"        # JIRA issue ID\n",
    "    ])\n",
    "    \n",
    "    # Additional optional fields\n",
    "    optional_fields: List[str] = field(default_factory=lambda: [\n",
    "        \"status\",\n",
    "        \"priority\", \n",
    "        \"assignee\",\n",
    "        \"reporter\",\n",
    "        \"issue_type\",\n",
    "        \"components\",\n",
    "        \"labels\",\n",
    "        \"resolution\",\n",
    "        \"updated_date\"\n",
    "    ])\n",
    "    \n",
    "    # JQL Query for filtering tickets\n",
    "    jql_query: str = \"project IN (YOUR_PROJECT) ORDER BY created DESC\"\n",
    "    \n",
    "    # Embedding settings\n",
    "    embedding_model_name: str = \"text-embedding-005\"\n",
    "    embedding_dimension: int = 768\n",
    "    \n",
    "    # Batch processing settings\n",
    "    batch_size: int = 50\n",
    "    max_tickets: Optional[int] = None  # None for all tickets\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Load configuration from environment variables if not provided\"\"\"\n",
    "        if not self.jira_url:\n",
    "            self.jira_url = os.getenv(\"JIRA_URL\", \"\")\n",
    "        if not self.jira_username:\n",
    "            self.jira_username = os.getenv(\"JIRA_USERNAME\", \"\")\n",
    "        if not self.jira_api_token:\n",
    "            self.jira_api_token = os.getenv(\"JIRA_API_TOKEN\", \"\")\n",
    "        if not self.pinecone_api_key:\n",
    "            self.pinecone_api_key = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
    "        if not self.google_cloud_project:\n",
    "            self.google_cloud_project = os.getenv(\"GOOGLE_CLOUD_PROJECT\", \"\")\n",
    "    \n",
    "    def validate(self) -> List[str]:\n",
    "        \"\"\"Validate configuration and return list of missing required fields\"\"\"\n",
    "        missing = []\n",
    "        if not self.jira_url:\n",
    "            missing.append(\"JIRA_URL\")\n",
    "        if not self.jira_username:\n",
    "            missing.append(\"JIRA_USERNAME\") \n",
    "        if not self.jira_api_token:\n",
    "            missing.append(\"JIRA_API_TOKEN\")\n",
    "        if not self.pinecone_api_key:\n",
    "            missing.append(\"PINECONE_API_KEY\")\n",
    "        if not self.google_cloud_project:\n",
    "            missing.append(\"GOOGLE_CLOUD_PROJECT\")\n",
    "        return missing\n",
    "\n",
    "# Create default configuration\n",
    "config = JiraIndexConfig()\n",
    "missing_fields = config.validate()\n",
    "\n",
    "if missing_fields:\n",
    "    print(\"âŒ Missing required environment variables:\")\n",
    "    for field in missing_fields:\n",
    "        print(f\"   - {field}\")\n",
    "    print(\"\\nğŸ’¡ Please set these in your .env file or directly in the config object.\")\n",
    "else:\n",
    "    print(\"âœ… Configuration validated successfully!\")\n",
    "    print(f\"ğŸ“Š Will index these fields: {', '.join(config.fields_to_index)}\")\n",
    "    print(f\"ğŸ—‚ï¸  Optional fields: {', '.join(config.optional_fields)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JiraClient:\n",
    "    \"\"\"Client for connecting to JIRA and fetching ticket data\"\"\"\n",
    "    \n",
    "    def __init__(self, config: JiraIndexConfig):\n",
    "        self.config = config\n",
    "        self.base_url = config.jira_url.rstrip('/')\n",
    "        self.auth = HTTPBasicAuth(config.jira_username, config.jira_api_token)\n",
    "        self.session = requests.Session()\n",
    "        self.session.auth = self.auth\n",
    "        \n",
    "    def test_connection(self) -> bool:\n",
    "        \"\"\"Test JIRA connection\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/rest/api/2/myself\")\n",
    "            if response.status_code == 200:\n",
    "                user_info = response.json()\n",
    "                print(f\"âœ… Connected to JIRA as: {user_info.get('displayName', 'Unknown')}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"âŒ JIRA connection failed: {response.status_code} - {response.text}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ JIRA connection error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_ticket_count(self, jql: str) -> int:\n",
    "        \"\"\"Get total count of tickets matching JQL query\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(\n",
    "                f\"{self.base_url}/rest/api/2/search\",\n",
    "                params={\"jql\": jql, \"maxResults\": 0}\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                return response.json().get(\"total\", 0)\n",
    "            else:\n",
    "                print(f\"âŒ Failed to get ticket count: {response.status_code}\")\n",
    "                return 0\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error getting ticket count: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def fetch_tickets_batch(self, jql: str, start_at: int = 0, max_results: int = 50) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Fetch a batch of tickets from JIRA\"\"\"\n",
    "        try:\n",
    "            # Define fields to fetch (always include key fields plus configured ones)\n",
    "            fields = [\"key\", \"id\", \"summary\", \"description\", \"created\", \"updated\"]\n",
    "            \n",
    "            # Add optional fields that are commonly available\n",
    "            optional_jira_fields = [\n",
    "                \"status\", \"priority\", \"assignee\", \"reporter\", \n",
    "                \"issuetype\", \"components\", \"labels\", \"resolution\"\n",
    "            ]\n",
    "            fields.extend(optional_jira_fields)\n",
    "            \n",
    "            params = {\n",
    "                \"jql\": jql,\n",
    "                \"startAt\": start_at,\n",
    "                \"maxResults\": max_results,\n",
    "                \"fields\": \",\".join(fields)\n",
    "            }\n",
    "            \n",
    "            response = self.session.get(f\"{self.base_url}/rest/api/2/search\", params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return data.get(\"issues\", [])\n",
    "            else:\n",
    "                print(f\"âŒ Failed to fetch tickets: {response.status_code} - {response.text}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error fetching tickets: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_ticket_data(self, jira_issue: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Extract and normalize ticket data based on configuration\"\"\"\n",
    "        fields = jira_issue.get(\"fields\", {})\n",
    "        ticket_data = {}\n",
    "        \n",
    "        # Map JIRA fields to our configured field names\n",
    "        field_mapping = {\n",
    "            \"ticket_number\": jira_issue.get(\"key\", \"\"),\n",
    "            \"ticket_id\": jira_issue.get(\"id\", \"\"),\n",
    "            \"title\": fields.get(\"summary\", \"\"),\n",
    "            \"summary\": fields.get(\"summary\", \"\"),  # Alternative name for title\n",
    "            \"description\": fields.get(\"description\", \"\") or \"\",\n",
    "            \"created_date\": fields.get(\"created\", \"\"),\n",
    "            \"updated_date\": fields.get(\"updated\", \"\"),\n",
    "            \"status\": fields.get(\"status\", {}).get(\"name\", \"\") if fields.get(\"status\") else \"\",\n",
    "            \"priority\": fields.get(\"priority\", {}).get(\"name\", \"\") if fields.get(\"priority\") else \"\",\n",
    "            \"assignee\": fields.get(\"assignee\", {}).get(\"displayName\", \"\") if fields.get(\"assignee\") else \"Unassigned\",\n",
    "            \"reporter\": fields.get(\"reporter\", {}).get(\"displayName\", \"\") if fields.get(\"reporter\") else \"\",\n",
    "            \"issue_type\": fields.get(\"issuetype\", {}).get(\"name\", \"\") if fields.get(\"issuetype\") else \"\",\n",
    "            \"components\": \", \".join([c.get(\"name\", \"\") for c in fields.get(\"components\", [])]),\n",
    "            \"labels\": \", \".join(fields.get(\"labels\", [])),\n",
    "            \"resolution\": fields.get(\"resolution\", {}).get(\"name\", \"\") if fields.get(\"resolution\") else \"\"\n",
    "        }\n",
    "        \n",
    "        # Include only configured fields in the final data\n",
    "        all_configured_fields = self.config.fields_to_index + self.config.optional_fields\n",
    "        for field_name in all_configured_fields:\n",
    "            if field_name in field_mapping:\n",
    "                ticket_data[field_name] = field_mapping[field_name]\n",
    "        \n",
    "        return ticket_data\n",
    "    \n",
    "    def create_text_content(self, ticket_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Create searchable text content from ticket data\"\"\"\n",
    "        # Prioritize the configured fields for text content\n",
    "        content_parts = []\n",
    "        \n",
    "        # Always include title/summary first if available\n",
    "        if ticket_data.get(\"title\") or ticket_data.get(\"summary\"):\n",
    "            title = ticket_data.get(\"title\") or ticket_data.get(\"summary\")\n",
    "            content_parts.append(f\"Title: {title}\")\n",
    "        \n",
    "        # Add description if configured and available\n",
    "        if \"description\" in self.config.fields_to_index and ticket_data.get(\"description\"):\n",
    "            content_parts.append(f\"Description: {ticket_data['description']}\")\n",
    "        \n",
    "        # Add other configured fields\n",
    "        for field in self.config.fields_to_index:\n",
    "            if field in [\"title\", \"summary\", \"description\"]:\n",
    "                continue  # Already handled above\n",
    "            if ticket_data.get(field):\n",
    "                field_label = field.replace(\"_\", \" \").title()\n",
    "                content_parts.append(f\"{field_label}: {ticket_data[field]}\")\n",
    "        \n",
    "        return \" | \".join(content_parts)\n",
    "\n",
    "# Initialize JIRA client\n",
    "if not missing_fields:\n",
    "    jira_client = JiraClient(config)\n",
    "    if jira_client.test_connection():\n",
    "        print(\"ğŸ‰ JIRA client initialized successfully!\")\n",
    "    else:\n",
    "        print(\"âŒ Failed to initialize JIRA client\")\n",
    "else:\n",
    "    print(\"â¸ï¸ Skipping JIRA client initialization due to missing configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7756c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexAIEmbeddingClient:\n",
    "    \"\"\"Client for generating embeddings using Google's Vertex AI\"\"\"\n",
    "    \n",
    "    def __init__(self, config: JiraIndexConfig):\n",
    "        self.config = config\n",
    "        self.embedding_model = None\n",
    "        self.initialize_vertex_ai()\n",
    "    \n",
    "    def initialize_vertex_ai(self):\n",
    "        \"\"\"Initialize Vertex AI and embedding model\"\"\"\n",
    "        try:\n",
    "            # Initialize Vertex AI with project\n",
    "            vertexai.init(project=self.config.google_cloud_project)\n",
    "            \n",
    "            # Load the embedding model\n",
    "            self.embedding_model = TextEmbeddingModel.from_pretrained(self.config.embedding_model_name)\n",
    "            \n",
    "            print(f\"âœ… Vertex AI initialized with model: {self.config.embedding_model_name}\")\n",
    "            print(f\"ğŸ—ï¸ Project: {self.config.google_cloud_project}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to initialize Vertex AI: {e}\")\n",
    "            print(\"ğŸ’¡ Make sure you have set up Google Cloud authentication and have the correct project ID\")\n",
    "    \n",
    "    def test_embedding(self) -> bool:\n",
    "        \"\"\"Test embedding generation with a sample text\"\"\"\n",
    "        if not self.embedding_model:\n",
    "            print(\"âŒ Embedding model not initialized\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            test_text = \"This is a test ticket about login issues\"\n",
    "            embeddings = self.embedding_model.get_embeddings([test_text])\n",
    "            \n",
    "            if embeddings and len(embeddings) > 0:\n",
    "                embedding_vector = embeddings[0].values\n",
    "                print(f\"âœ… Embedding test successful!\")\n",
    "                print(f\"ğŸ“Š Embedding dimension: {len(embedding_vector)}\")\n",
    "                print(f\"ğŸ”¢ Sample values: {embedding_vector[:5]}...\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âŒ Embedding test failed: No embeddings returned\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Embedding test failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for a batch of texts\"\"\"\n",
    "        if not self.embedding_model:\n",
    "            raise Exception(\"Embedding model not initialized\")\n",
    "        \n",
    "        try:\n",
    "            # Vertex AI embedding model can handle multiple texts at once\n",
    "            embeddings = self.embedding_model.get_embeddings(texts)\n",
    "            return [embedding.values for embedding in embeddings]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error generating embeddings: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def generate_single_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for a single text\"\"\"\n",
    "        embeddings = self.generate_embeddings_batch([text])\n",
    "        return embeddings[0] if embeddings else []\n",
    "\n",
    "# Initialize Vertex AI client\n",
    "if not missing_fields:\n",
    "    vertex_client = VertexAIEmbeddingClient(config)\n",
    "    if vertex_client.test_embedding():\n",
    "        print(\"ğŸ‰ Vertex AI embedding client ready!\")\n",
    "    else:\n",
    "        print(\"âŒ Vertex AI embedding client initialization failed\")\n",
    "else:\n",
    "    print(\"â¸ï¸ Skipping Vertex AI client initialization due to missing configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeClient:\n",
    "    \"\"\"Client for Pinecone vector database operations\"\"\"\n",
    "    \n",
    "    def __init__(self, config: JiraIndexConfig):\n",
    "        self.config = config\n",
    "        self.pc = None\n",
    "        self.index = None\n",
    "        self.initialize_pinecone()\n",
    "    \n",
    "    def initialize_pinecone(self):\n",
    "        \"\"\"Initialize Pinecone client and connect to index\"\"\"\n",
    "        try:\n",
    "            # Initialize Pinecone\n",
    "            self.pc = Pinecone(api_key=self.config.pinecone_api_key)\n",
    "            print(\"âœ… Pinecone client initialized\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to initialize Pinecone: {e}\")\n",
    "    \n",
    "    def create_index_if_not_exists(self) -> bool:\n",
    "        \"\"\"Create Pinecone index if it doesn't exist\"\"\"\n",
    "        if not self.pc:\n",
    "            print(\"âŒ Pinecone client not initialized\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Check if index already exists\n",
    "            existing_indexes = [idx.name for idx in self.pc.list_indexes()]\n",
    "            \n",
    "            if self.config.pinecone_index_name in existing_indexes:\n",
    "                print(f\"âœ… Index '{self.config.pinecone_index_name}' already exists\")\n",
    "                self.index = self.pc.Index(self.config.pinecone_index_name)\n",
    "                return True\n",
    "            \n",
    "            # Create new index\n",
    "            print(f\"ğŸ—ï¸ Creating new index: {self.config.pinecone_index_name}\")\n",
    "            self.pc.create_index(\n",
    "                name=self.config.pinecone_index_name,\n",
    "                dimension=self.config.embedding_dimension,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"gcp\",  # or \"aws\" \n",
    "                    region=\"us-central1\"  # adjust based on your preference\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Wait for index to be ready\n",
    "            print(\"â³ Waiting for index to be ready...\")\n",
    "            time.sleep(30)  # Give index time to initialize\n",
    "            \n",
    "            self.index = self.pc.Index(self.config.pinecone_index_name)\n",
    "            print(f\"âœ… Index '{self.config.pinecone_index_name}' created successfully!\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error creating index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_index_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the index\"\"\"\n",
    "        if not self.index:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            stats = self.index.describe_index_stats()\n",
    "            return stats\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error getting index stats: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def upsert_vectors_batch(self, vectors_data: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"Upsert a batch of vectors to Pinecone\"\"\"\n",
    "        if not self.index:\n",
    "            print(\"âŒ Index not initialized\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Prepare vectors for upsert\n",
    "            vectors_to_upsert = []\n",
    "            for data in vectors_data:\n",
    "                vector_id = data.get(\"id\")\n",
    "                embedding = data.get(\"embedding\")\n",
    "                metadata = data.get(\"metadata\", {})\n",
    "                \n",
    "                if vector_id and embedding:\n",
    "                    vectors_to_upsert.append({\n",
    "                        \"id\": vector_id,\n",
    "                        \"values\": embedding,\n",
    "                        \"metadata\": metadata\n",
    "                    })\n",
    "            \n",
    "            if vectors_to_upsert:\n",
    "                self.index.upsert(vectors=vectors_to_upsert)\n",
    "                print(f\"âœ… Upserted {len(vectors_to_upsert)} vectors to Pinecone\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âš ï¸ No valid vectors to upsert\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error upserting vectors: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def query_similar_vectors(self, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Query for similar vectors\"\"\"\n",
    "        if not self.index:\n",
    "            print(\"âŒ Index not initialized\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            results = self.index.query(\n",
    "                vector=query_embedding,\n",
    "                top_k=top_k,\n",
    "                include_metadata=True,\n",
    "                include_values=False\n",
    "            )\n",
    "            \n",
    "            return [{\n",
    "                \"id\": match.id,\n",
    "                \"score\": match.score,\n",
    "                \"metadata\": match.metadata\n",
    "            } for match in results.matches]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error querying vectors: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def delete_all_vectors(self) -> bool:\n",
    "        \"\"\"Delete all vectors from the index (use with caution!)\"\"\"\n",
    "        if not self.index:\n",
    "            print(\"âŒ Index not initialized\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self.index.delete(delete_all=True)\n",
    "            print(\"âœ… All vectors deleted from index\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error deleting vectors: {e}\")\n",
    "            return False\n",
    "\n",
    "# Initialize Pinecone client\n",
    "if not missing_fields:\n",
    "    pinecone_client = PineconeClient(config)\n",
    "    if pinecone_client.create_index_if_not_exists():\n",
    "        stats = pinecone_client.get_index_stats()\n",
    "        print(f\"ğŸ“Š Index stats: {stats}\")\n",
    "        print(\"ğŸ‰ Pinecone client ready!\")\n",
    "    else:\n",
    "        print(\"âŒ Pinecone client initialization failed\")\n",
    "else:\n",
    "    print(\"â¸ï¸ Skipping Pinecone client initialization due to missing configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JiraPineconeIndexer:\n",
    "    \"\"\"Main orchestrator for indexing JIRA tickets to Pinecone\"\"\"\n",
    "    \n",
    "    def __init__(self, config: JiraIndexConfig):\n",
    "        self.config = config\n",
    "        self.jira_client = JiraClient(config)\n",
    "        self.vertex_client = VertexAIEmbeddingClient(config)\n",
    "        self.pinecone_client = PineconeClient(config)\n",
    "        \n",
    "    def validate_setup(self) -> bool:\n",
    "        \"\"\"Validate that all clients are properly initialized\"\"\"\n",
    "        checks = [\n",
    "            (\"JIRA\", self.jira_client.test_connection()),\n",
    "            (\"Vertex AI\", self.vertex_client.embedding_model is not None),\n",
    "            (\"Pinecone\", self.pinecone_client.index is not None)\n",
    "        ]\n",
    "        \n",
    "        all_good = True\n",
    "        for service, status in checks:\n",
    "            if status:\n",
    "                print(f\"âœ… {service}: Ready\")\n",
    "            else:\n",
    "                print(f\"âŒ {service}: Not ready\")\n",
    "                all_good = False\n",
    "        \n",
    "        return all_good\n",
    "    \n",
    "    def preview_tickets(self, max_preview: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Preview a few tickets to see what data will be indexed\"\"\"\n",
    "        print(f\"ğŸ” Previewing {max_preview} tickets with current configuration...\")\n",
    "        print(f\"ğŸ“‹ Fields to index: {', '.join(self.config.fields_to_index)}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        tickets = self.jira_client.fetch_tickets_batch(\n",
    "            self.config.jql_query, \n",
    "            start_at=0, \n",
    "            max_results=max_preview\n",
    "        )\n",
    "        \n",
    "        previews = []\n",
    "        for i, ticket in enumerate(tickets, 1):\n",
    "            ticket_data = self.jira_client.extract_ticket_data(ticket)\n",
    "            text_content = self.jira_client.create_text_content(ticket_data)\n",
    "            \n",
    "            print(f\"\\nğŸ« Ticket {i}: {ticket_data.get('ticket_number', 'Unknown')}\")\n",
    "            print(f\"ğŸ“ Text content preview:\")\n",
    "            print(f\"   {text_content[:200]}{'...' if len(text_content) > 200 else ''}\")\n",
    "            print(f\"ğŸ·ï¸ Metadata fields: {list(ticket_data.keys())}\")\n",
    "            \n",
    "            previews.append({\n",
    "                'ticket_data': ticket_data,\n",
    "                'text_content': text_content\n",
    "            })\n",
    "        \n",
    "        return previews\n",
    "    \n",
    "    def index_all_tickets(self, dry_run: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Index all JIRA tickets to Pinecone\"\"\"\n",
    "        if not self.validate_setup():\n",
    "            return {\"success\": False, \"error\": \"Setup validation failed\"}\n",
    "        \n",
    "        print(\"ğŸš€ Starting JIRA ticket indexing process...\")\n",
    "        \n",
    "        # Get total ticket count\n",
    "        total_tickets = self.jira_client.get_ticket_count(self.config.jql_query)\n",
    "        print(f\"ğŸ“Š Total tickets to process: {total_tickets}\")\n",
    "        \n",
    "        if self.config.max_tickets:\n",
    "            total_tickets = min(total_tickets, self.config.max_tickets)\n",
    "            print(f\"âš¡ Limited to: {total_tickets} tickets\")\n",
    "        \n",
    "        if dry_run:\n",
    "            print(\"ğŸ§ª DRY RUN MODE - No data will be written to Pinecone\")\n",
    "        \n",
    "        # Process tickets in batches\n",
    "        start_time = time.time()\n",
    "        processed_count = 0\n",
    "        error_count = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for start_at in range(0, total_tickets, self.config.batch_size):\n",
    "            batch_count += 1\n",
    "            end_at = min(start_at + self.config.batch_size, total_tickets)\n",
    "            \n",
    "            print(f\"\\nğŸ“¦ Processing batch {batch_count}: tickets {start_at + 1}-{end_at}\")\n",
    "            \n",
    "            # Fetch ticket batch\n",
    "            tickets = self.jira_client.fetch_tickets_batch(\n",
    "                self.config.jql_query,\n",
    "                start_at=start_at,\n",
    "                max_results=self.config.batch_size\n",
    "            )\n",
    "            \n",
    "            if not tickets:\n",
    "                print(\"âš ï¸ No tickets returned, stopping\")\n",
    "                break\n",
    "            \n",
    "            # Process batch\n",
    "            batch_vectors = []\n",
    "            for ticket in tickets:\n",
    "                try:\n",
    "                    # Extract ticket data\n",
    "                    ticket_data = self.jira_client.extract_ticket_data(ticket)\n",
    "                    text_content = self.jira_client.create_text_content(ticket_data)\n",
    "                    \n",
    "                    if not text_content.strip():\n",
    "                        print(f\"âš ï¸ Skipping {ticket_data.get('ticket_number', 'Unknown')}: No content\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Generate embedding\n",
    "                    embedding = self.vertex_client.generate_single_embedding(text_content)\n",
    "                    \n",
    "                    if not embedding:\n",
    "                        print(f\"âš ï¸ Failed to generate embedding for {ticket_data.get('ticket_number', 'Unknown')}\")\n",
    "                        error_count += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Prepare vector data\n",
    "                    vector_data = {\n",
    "                        \"id\": ticket_data.get(\"ticket_number\", f\"ticket-{ticket_data.get('ticket_id', 'unknown')}\"),\n",
    "                        \"embedding\": embedding,\n",
    "                        \"metadata\": {\n",
    "                            **ticket_data,\n",
    "                            \"text_content\": text_content[:1000],  # Limit metadata size\n",
    "                            \"indexed_at\": datetime.now(timezone.utc).isoformat()\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    batch_vectors.append(vector_data)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error processing ticket: {e}\")\n",
    "                    error_count += 1\n",
    "            \n",
    "            # Upsert batch to Pinecone\n",
    "            if batch_vectors and not dry_run:\n",
    "                success = self.pinecone_client.upsert_vectors_batch(batch_vectors)\n",
    "                if not success:\n",
    "                    print(f\"âŒ Failed to upsert batch {batch_count}\")\n",
    "            elif dry_run:\n",
    "                print(f\"ğŸ§ª Would upsert {len(batch_vectors)} vectors (dry run)\")\n",
    "            \n",
    "            # Progress update\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = processed_count / elapsed if elapsed > 0 else 0\n",
    "            print(f\"ğŸ“ˆ Progress: {processed_count}/{total_tickets} processed ({rate:.1f} tickets/sec)\")\n",
    "        \n",
    "        # Final summary\n",
    "        total_time = time.time() - start_time\n",
    "        summary = {\n",
    "            \"success\": True,\n",
    "            \"total_processed\": processed_count,\n",
    "            \"total_errors\": error_count,\n",
    "            \"total_time_seconds\": total_time,\n",
    "            \"processing_rate\": processed_count / total_time if total_time > 0 else 0,\n",
    "            \"dry_run\": dry_run\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Indexing complete!\")\n",
    "        print(f\"âœ… Processed: {processed_count} tickets\")\n",
    "        print(f\"âŒ Errors: {error_count}\")\n",
    "        print(f\"â±ï¸ Total time: {total_time:.1f} seconds\")\n",
    "        print(f\"ğŸ“Š Rate: {summary['processing_rate']:.1f} tickets/second\")\n",
    "        \n",
    "        if not dry_run:\n",
    "            # Get final index stats\n",
    "            final_stats = self.pinecone_client.get_index_stats()\n",
    "            print(f\"ğŸ“ˆ Final index stats: {final_stats}\")\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def test_search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Test search functionality with a query\"\"\"\n",
    "        if not self.validate_setup():\n",
    "            return []\n",
    "        \n",
    "        print(f\"ğŸ” Testing search with query: '{query}'\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.vertex_client.generate_single_embedding(query)\n",
    "        if not query_embedding:\n",
    "            print(\"âŒ Failed to generate query embedding\")\n",
    "            return []\n",
    "        \n",
    "        # Search Pinecone\n",
    "        results = self.pinecone_client.query_similar_vectors(query_embedding, top_k)\n",
    "        \n",
    "        print(f\"ğŸ“‹ Found {len(results)} similar tickets:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            metadata = result.get(\"metadata\", {})\n",
    "            print(f\"\\n{i}. {metadata.get('ticket_number', 'Unknown')} (Score: {result['score']:.3f})\")\n",
    "            print(f\"   ğŸ“ {metadata.get('title', metadata.get('summary', 'No title'))}\")\n",
    "            print(f\"   ğŸ¯ {metadata.get('text_content', '')[:100]}...\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize the main indexer\n",
    "if not missing_fields:\n",
    "    indexer = JiraPineconeIndexer(config)\n",
    "    print(\"ğŸ‰ JIRA Pinecone Indexer initialized successfully!\")\n",
    "    print(\"ğŸ’¡ You can now run indexer.preview_tickets() to see what will be indexed\")\n",
    "else:\n",
    "    print(\"â¸ï¸ Skipping indexer initialization due to missing configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea2c8c",
   "metadata": {},
   "source": [
    "# ğŸš€ JIRA to Pinecone Vector Database Setup Complete!\n",
    "\n",
    "## âœ… What We've Built\n",
    "\n",
    "This notebook provides a complete solution for:\n",
    "\n",
    "1. **ğŸ”— JIRA Connection**: Connects to JIRA using REST API with configurable authentication\n",
    "2. **ğŸ§  Vertex AI Embeddings**: Uses Google's `text-embedding-005` model for high-quality embeddings  \n",
    "3. **ğŸ—‚ï¸ Pinecone Vector DB**: Stores and indexes ticket embeddings for semantic search\n",
    "4. **âš™ï¸ Configurable Fields**: Customize which JIRA fields to index (title, description, summary, created_date, ticket_number, etc.)\n",
    "\n",
    "## ğŸ› ï¸ Setup Instructions\n",
    "\n",
    "### 1. Environment Variables Required\n",
    "\n",
    "Create a `.env` file in your project root with:\n",
    "\n",
    "```bash\n",
    "# JIRA Configuration\n",
    "JIRA_URL=https://your-domain.atlassian.net\n",
    "JIRA_USERNAME=your-email@company.com  \n",
    "JIRA_API_TOKEN=your-jira-api-token\n",
    "\n",
    "# Pinecone Configuration\n",
    "PINECONE_API_KEY=your-pinecone-api-key\n",
    "PINECONE_INDEX_NAME=jira-tickets\n",
    "\n",
    "# Google Cloud Configuration  \n",
    "GOOGLE_CLOUD_PROJECT=your-gcp-project-id\n",
    "```\n",
    "\n",
    "### 2. Field Configuration\n",
    "\n",
    "Customize which JIRA fields to index by modifying the `config.fields_to_index` list:\n",
    "\n",
    "```python\n",
    "# Default configured fields\n",
    "config.fields_to_index = [\n",
    "    \"title\",           # Issue summary/title\n",
    "    \"description\",     # Issue description  \n",
    "    \"summary\",         # Alternative to title\n",
    "    \"created_date\",    # When ticket was created\n",
    "    \"ticket_number\",   # JIRA key (e.g., PROJ-123)\n",
    "    \"ticket_id\"        # JIRA issue ID\n",
    "]\n",
    "\n",
    "# Optional additional fields\n",
    "config.optional_fields = [\n",
    "    \"status\", \"priority\", \"assignee\", \"reporter\", \n",
    "    \"issue_type\", \"components\", \"labels\", \"resolution\", \"updated_date\"\n",
    "]\n",
    "```\n",
    "\n",
    "### 3. JQL Query Customization\n",
    "\n",
    "Modify the JQL query to filter which tickets to index:\n",
    "\n",
    "```python\n",
    "config.jql_query = \"project IN (MYPROJ, OTHERPROJ) AND created >= -30d ORDER BY created DESC\"\n",
    "```\n",
    "\n",
    "## ğŸ¯ Usage Examples\n",
    "\n",
    "The next cells will show you how to use the indexer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f02eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‹ Step 1: Customize Configuration (Optional)\n",
    "# Modify these settings based on your needs\n",
    "\n",
    "if 'config' in locals():\n",
    "    # Example: Customize fields to index\n",
    "    config.fields_to_index = [\n",
    "        \"title\",\n",
    "        \"description\", \n",
    "        \"ticket_number\",\n",
    "        \"created_date\",\n",
    "        \"summary\"\n",
    "    ]\n",
    "    \n",
    "    # Example: Customize JQL query\n",
    "    config.jql_query = \"project = YOUR_PROJECT ORDER BY created DESC\"\n",
    "    \n",
    "    # Example: Limit number of tickets for testing\n",
    "    config.max_tickets = 100  # Remove or set to None for all tickets\n",
    "    \n",
    "    print(\"âœ… Configuration updated!\")\n",
    "    print(f\"ğŸ¯ Fields to index: {config.fields_to_index}\")\n",
    "    print(f\"ğŸ” JQL Query: {config.jql_query}\")\n",
    "    if config.max_tickets:\n",
    "        print(f\"ğŸ“Š Max tickets: {config.max_tickets}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Configuration not available - check previous cells for errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc64a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Step 2: Preview Tickets\n",
    "# See what tickets will be indexed and how they'll be processed\n",
    "\n",
    "if 'indexer' in locals():\n",
    "    print(\"ğŸ” Previewing tickets that will be indexed...\")\n",
    "    preview_results = indexer.preview_tickets(max_preview=3)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Preview complete! {len(preview_results)} tickets shown.\")\n",
    "    print(\"ğŸ’¡ If this looks good, proceed to the next step to start indexing.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Indexer not available - check previous cells for errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be876141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª Step 3: Dry Run (Recommended)\n",
    "# Test the full process without writing to Pinecone\n",
    "\n",
    "if 'indexer' in locals():\n",
    "    print(\"ğŸ§ª Running dry run - no data will be written to Pinecone\")\n",
    "    print(\"This helps validate the process before running the real indexing\")\n",
    "    \n",
    "    dry_run_results = indexer.index_all_tickets(dry_run=True)\n",
    "    \n",
    "    if dry_run_results.get(\"success\"):\n",
    "        print(\"\\nâœ… Dry run completed successfully!\")\n",
    "        print(f\"ğŸ“Š Would process {dry_run_results['total_processed']} tickets\")\n",
    "        print(f\"âŒ {dry_run_results['total_errors']} errors encountered\")\n",
    "        print(f\"â±ï¸ Estimated time: {dry_run_results['total_time_seconds']:.1f} seconds\")\n",
    "        print(\"ğŸš€ Ready to run the real indexing!\")\n",
    "    else:\n",
    "        print(\"âŒ Dry run failed:\", dry_run_results.get(\"error\", \"Unknown error\"))\n",
    "else:\n",
    "    print(\"âš ï¸ Indexer not available - check previous cells for errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ Step 4: Run Real Indexing\n",
    "# âš ï¸ This will actually create embeddings and store them in Pinecone\n",
    "\n",
    "if 'indexer' in locals():\n",
    "    print(\"ğŸš€ Starting real indexing process...\")\n",
    "    print(\"âš ï¸ This will consume Vertex AI API credits and store data in Pinecone\")\n",
    "    \n",
    "    # Uncomment the line below when you're ready to run the real indexing\n",
    "    # results = indexer.index_all_tickets(dry_run=False)\n",
    "    \n",
    "    print(\"ğŸ›‘ Real indexing is commented out for safety.\")\n",
    "    print(\"ğŸ’¡ Uncomment the line above when you're ready to proceed.\")\n",
    "    print(\"ğŸ“‹ Make sure you've reviewed the dry run results first!\")\n",
    "    \n",
    "    # If you want to run it, uncomment these lines:\n",
    "    # if results.get(\"success\"):\n",
    "    #     print(f\"âœ… Indexing completed! Processed {results['total_processed']} tickets\")\n",
    "    # else:\n",
    "    #     print(f\"âŒ Indexing failed: {results.get('error', 'Unknown error')}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Indexer not available - check previous cells for errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13964f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Step 5: Test Search Functionality\n",
    "# Test semantic search on your indexed tickets\n",
    "\n",
    "if 'indexer' in locals():\n",
    "    # Example search queries - modify these based on your tickets\n",
    "    test_queries = [\n",
    "        \"login authentication issues\",\n",
    "        \"database performance slow\",\n",
    "        \"UI button overlapping mobile\",\n",
    "        \"API endpoint returning error\",\n",
    "        \"security vulnerability file upload\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ” Testing search functionality with sample queries...\")\n",
    "    print(\"ğŸ’¡ These queries use semantic similarity to find relevant tickets\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nğŸ¯ Testing query: '{query}'\")\n",
    "        results = indexer.test_search(query, top_k=2)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"   ğŸ“­ No results found\")\n",
    "        print(\"   \" + \"-\" * 50)\n",
    "    \n",
    "    print(\"\\nâœ… Search testing complete!\")\n",
    "    print(\"ğŸ’¡ You can now use indexer.test_search('your query') for custom searches\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Indexer not available - check previous cells for errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15402d87",
   "metadata": {},
   "source": [
    "# ğŸ‰ JIRA to Pinecone Vector Database Complete!\n",
    "\n",
    "## âœ… What You've Accomplished\n",
    "\n",
    "1. **ğŸ”— Connected to JIRA** - Authenticated and tested connection to your JIRA instance\n",
    "2. **ğŸ§  Configured Vertex AI** - Set up Google's text-embedding-005 model for embeddings\n",
    "3. **ğŸ—‚ï¸ Set up Pinecone** - Created/connected to vector database for storage\n",
    "4. **âš™ï¸ Customized Fields** - Configured which JIRA ticket fields to index\n",
    "5. **ğŸ” Tested Search** - Validated semantic search functionality\n",
    "\n",
    "## ğŸ¯ Configurable Fields Used\n",
    "\n",
    "The indexer is configured to use these JIRA ticket fields:\n",
    "\n",
    "- **title/summary** - The main ticket title\n",
    "- **description** - Detailed ticket description  \n",
    "- **ticket_number** - JIRA key (e.g., PROJ-123)\n",
    "- **ticket_id** - Internal JIRA ID\n",
    "- **created_date** - When the ticket was created\n",
    "- Plus optional fields like status, priority, assignee, etc.\n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "### For CrewAI Integration:\n",
    "Your existing `JiraTicketSearchTool` in `src/cria_crew/tools/custom_tool.py` is already set up to use this Pinecone index!\n",
    "\n",
    "### For Production Use:\n",
    "1. **Schedule Updates** - Set up periodic re-indexing for new tickets\n",
    "2. **Monitor Usage** - Track Vertex AI API usage and Pinecone storage\n",
    "3. **Optimize Queries** - Fine-tune JQL queries for your specific needs\n",
    "4. **Scale Up** - Remove `max_tickets` limit for full indexing\n",
    "\n",
    "### Custom Search:\n",
    "```python\n",
    "# Use this for custom searches\n",
    "results = indexer.test_search(\"your search query\", top_k=5)\n",
    "```\n",
    "\n",
    "### Environment Variables Required:\n",
    "```bash\n",
    "JIRA_URL=https://your-domain.atlassian.net\n",
    "JIRA_USERNAME=your-email@company.com\n",
    "JIRA_API_TOKEN=your-jira-api-token  \n",
    "PINECONE_API_KEY=your-pinecone-api-key\n",
    "GOOGLE_CLOUD_PROJECT=your-gcp-project-id\n",
    "```\n",
    "\n",
    "ğŸŠ **Your JIRA tickets are now searchable using semantic similarity!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cria-crew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
